# Multi-level Cognitive Architecture for Multimodal Tumor Analysis

This repository implements a **three-level cognitive architecture** for tumor
analysis and classification, integrating automated ROI segmentation, multimodal
feature extraction, and transformer-based feature fusion.

The framework is designed for **medical imaging applications** and emphasizes
modularity, interpretability, and reproducibility.

---

## ğŸ§  Overall Architecture

The proposed framework consists of three hierarchical modules:

Level I : ROI Automatic Segmentation (2D U-Net)
Level II : Multimodal Feature Extraction
Level III : ViT-based Multidimensional Feature Fusion and Classification

Each module is implemented (or documented) as an independent and reusable
component.

---

## ğŸ“‚ Repository Structure

â”œâ”€ A_ROI_Segmentation_2DUNet/
â”œâ”€ B_Multimodal_Feature_Extraction/
â”œâ”€ C_ViT_Fusion/
â””â”€ README.md

---

## ğŸ”¹ Module A: ROI Automatic Segmentation (2D U-Net)

**Path**: `A_ROI_Segmentation_2DUNet/`

Module A performs **automatic tumor ROI segmentation** from 3D medical images
(NIfTI format) using a **2D U-Net** trained on slice-wise data.

### Key Features
- 3D NIfTI â†’ 2D slice-based training
- Combined **BCEWithLogits + Dice loss**
- Data augmentation (flip, rotation)
- Inference with per-slice **overlay / probability / binary mask PNG outputs**

### Directory Overview
A_ROI_Segmentation_2DUNet/
â”œâ”€ src/
â”‚ â”œâ”€ datasets/ # NIfTI slice dataset
â”‚ â”œâ”€ models/ # 2D U-Net
â”‚ â”œâ”€ losses/ # Dice loss
â”‚ â”œâ”€ utils/ # Visualization tools
â”‚ â”œâ”€ train_seg.py
â”‚ â””â”€ infer_seg_png.py
â”œâ”€ docs/
â”‚ â”œâ”€ data_format.md # Data organization & mask definition
â”‚ â””â”€ demo_results/ # Example PNG results

> âš ï¸ Raw medical images (`.nii.gz`) and trained model weights are **not included**
> in this repository. See `docs/data_format.md` for data preparation details.

---

## ğŸ”¹ Module B: Multimodal Feature Extraction

**Path**: `B_Multimodal_Feature_Extraction/`

Module B corresponds to the **second-level cognitive architecture** and extracts
heterogeneous features from segmented tumor regions.

### Extracted Feature Modalities
1. **Radiomics features** (PyRadiomics)
2. **2D CNN features** (DenseNet121)
3. **3D CNN features** (ShuffleNet3D)
4. **Clinical variables** (structured tabular data)

### Implementation Note
- Radiomics extraction is based on the open-source **PyRadiomics** library.
- Deep learning feature extraction (DenseNet121, ShuffleNet3D) is implemented
  using **vendor-provided (onekey) code**, which is proprietary.
- Due to licensing restrictions, **source code for Module B is not redistributed**.

### What *Is* Provided
- Detailed methodological descriptions
- Feature definitions and preprocessing notes
- Licensing and reproducibility statements

B_Multimodal_Feature_Extraction/
â”œâ”€ README.md
â””â”€ docs/
â”œâ”€ overview.md
â”œâ”€ radiomics_features.md
â”œâ”€ deep_features.md
â”œâ”€ clinical_variables.md
â””â”€ licensing_and_disclaimer.md

> âœ… Any alternative implementation that produces **equivalent feature tables**
> can be used to reproduce downstream results.

---

## ğŸ”¹ Module C: ViT-based Multimodal Feature Fusion

**Path**: `C_ViT_Fusion/`

Module C implements the **core methodological contribution** of this work:
a **Vision Transformer (ViT)â€“based fusion network** that models cross-modality
relationships and performs final classification.

### Model Highlights
- Each modality is treated as a **token**
- Tokens + `[CLS]` are processed by a Transformer encoder
- Optional modality-type embeddings
- End-to-end trainable fusion and classification

### Directory Overview
C_ViT_Fusion/
â”œâ”€ src/
â”‚ â”œâ”€ data/ # CSV loading & Dataset
â”‚ â”œâ”€ models/ # ViT-based fusion network
â”‚ â”œâ”€ utils/ # Metrics, plots, reproducibility
â”‚ â””â”€ train_fusion.py
â”œâ”€ examples/
â”‚ â””â”€ demo_features/ # Synthetic / anonymized demo CSVs
â””â”€ docs/
â””â”€ feature_interface.md

### Demo Feature Files
A minimal set of **example feature CSVs** is provided for smoke testing:

examples/demo_features/
â”œâ”€ labeRND-0-group.csv
â”œâ”€ radiomics_selected.csv
â”œâ”€ feat2d_selected.csv
â”œâ”€ feat3d_selected.csv
â””â”€ clinical.csv

These files:
- Do **not** contain patient-identifying information
- Are intended only to verify that the fusion pipeline runs correctly

---

## ğŸ” Reproducibility Strategy

- **Module A**: Fully reproducible with user-provided NIfTI data
- **Module B**: Interface-level reproducibility (feature definitions & formats)
- **Module C**: Fully reproducible ViT-based fusion and training code

Researchers may substitute their own implementations for Module B as long as
the feature interface is respected.

---

## ğŸ“œ Notes on Data Privacy and Licensing

- No raw medical images are included in this repository.
- No proprietary source code is redistributed.
- Users are responsible for complying with local data protection regulations
  and third-party software licenses.

---

## ğŸ“§ Contact

For questions regarding methodology or reproduction, please contact the authors
through the corresponding publication.

---

**This repository is intended for academic research use only.**
